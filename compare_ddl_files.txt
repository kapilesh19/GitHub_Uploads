#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
oracle_ddl_compare_py27.py  (Python 2.7.5 compatible)

What it does:
- Uses markers to split each export file into 1 DDL block per object:
    START marker line:  "~~##~~
    END marker line:    ~~##~~"
- Writes split files into Prod/ and QAT/ folders:
    <ObjectType>_<ObjectName>.sql   (schema removed from filename)
- Creates CSV inventory:
    Prod Filename (Yes/No), QAT Filename (Yes/No), Object Type, User, Object Name, Comparison result
- If an object exists in BOTH Prod and QAT and differs (after normalization),
  writes ONE mismatch file per object into the folder passed via --mismatch-dir.
  (No mismatch file created if Match.)

Usage:
  python oracle_ddl_compare_py27.py --prod Prod_file.sql --qat QAT_file.sql --out objects.csv --mismatch-dir Diff
"""

import argparse
import csv
import os
import re

# -----------------------------
# Marker lines (exact lines)
# -----------------------------
START_RE = re.compile(r'^\s*"\s*~~##~~\s*$')   # start of each DDL block
END_RE   = re.compile(r'^\s*~~##~~\s*"\s*$')   # end of each DDL block

# -----------------------------
# Object header parsing
# -----------------------------
# NOTE: This looks for the first CREATE ... <TYPE> <NAME> inside the block.
OBJ_HEADER_RE = re.compile(
    r'CREATE\s+(?:OR\s+REPLACE\s+)?'
    r'(PACKAGE\s+BODY|TYPE\s+BODY|PACKAGE|TYPE|VIEW|TABLE|INDEX|SEQUENCE|PROCEDURE|FUNCTION|TRIGGER)\s+'
    r'((?:"[^"]+"|\w[\w$#]*)(?:\s*\.\s*(?:"[^"]+"|\w[\w$#]*))?)',
    re.IGNORECASE
)

COMMENT_LINE_RE  = re.compile(r"^\s*--.*?$", re.MULTILINE)
COMMENT_BLOCK_RE = re.compile(r"/\*.*?\*/", re.DOTALL)

# Stripping env-specific clauses to compare "logical" DDL
STRIP_PATTERNS = [
    re.compile(r"\bTABLESPACE\b\s+\w[\w$#]*", re.IGNORECASE),
    re.compile(r"\bSTORAGE\b\s*\(.*?\)", re.IGNORECASE | re.DOTALL),
    re.compile(r"\bPCTFREE\b\s+\d+", re.IGNORECASE),
    re.compile(r"\bPCTUSED\b\s+\d+", re.IGNORECASE),
    re.compile(r"\bINITRANS\b\s+\d+", re.IGNORECASE),
    re.compile(r"\bMAXTRANS\b\s+\d+", re.IGNORECASE),
    re.compile(r"\bNOLOGGING\b|\bLOGGING\b", re.IGNORECASE),
    re.compile(r"\bNOPARALLEL\b|\bPARALLEL\b(?:\s+\d+)?", re.IGNORECASE),
    re.compile(r"\bNOCOMPRESS\b|\bCOMPRESS\b(?:\s+\w+)?", re.IGNORECASE),
    re.compile(r"\bSEGMENT\s+CREATION\b\s+(?:IMMEDIATE|DEFERRED)", re.IGNORECASE),
    re.compile(r"\bCACHE\b\s+\d+|\bNOCACHE\b", re.IGNORECASE),
]

def ensure_dir(path):
    if path and (not os.path.exists(path)):
        os.makedirs(path)

def safe_filename(s):
    return re.sub(r"[^A-Za-z0-9_\-$#]+", "_", s.strip())

def unquote_ident(s):
    s = s.strip()
    if s.startswith('"') and s.endswith('"'):
        return s[1:-1]
    return s

def split_schema_object(qualified):
    q = qualified.strip()
    if "." in q:
        left, right = q.split(".", 1)
        return unquote_ident(left.strip()), unquote_ident(right.strip())
    return None, unquote_ident(q)

def normalize_sql(text, strip_physical=True):
    t = COMMENT_BLOCK_RE.sub("", text)
    t = COMMENT_LINE_RE.sub("", t)
    t = t.replace("\r\n", "\n").replace("\r", "\n")
    if strip_physical:
        for pat in STRIP_PATTERNS:
            t = pat.sub("", t)
    t = re.sub(r"\s+", " ", t).strip()
    return t.upper()

def split_blocks_by_markers(sql_text):
    lines = sql_text.replace("\r\n", "\n").replace("\r", "\n").split("\n")
    blocks = []
    buf = []
    inside = False

    for line in lines:
        if (not inside) and START_RE.match(line):
            inside = True
            buf = []
            continue

        if inside and END_RE.match(line):
            block = "\n".join(buf).strip()
            if block:
                blocks.append(block)
            inside = False
            buf = []
            continue

        if inside:
            buf.append(line)

    # If file ends while still inside a block, keep it
    if inside:
        block = "\n".join(buf).strip()
        if block:
            blocks.append(block)

    return blocks

def extract_object(block):
    """
    Returns ((obj_type, obj_name), user) or None.
    Key ignores schema to avoid duplicate rows.
    """
    m = OBJ_HEADER_RE.search(block)
    if not m:
        return None

    obj_type = re.sub(r"\s+", " ", m.group(1).strip()).upper()
    qualified = m.group(2).strip()

    schema, name = split_schema_object(qualified)
    user = schema if schema else "UNKNOWN"
    key = (obj_type, name)
    return key, user

def read_file(path):
    f = open(path, "rb")
    try:
        return f.read()
    finally:
        f.close()

def write_file(path, content):
    f = open(path, "wb")
    try:
        if isinstance(content, unicode):
            content = content.encode("utf-8")
        f.write(content)
    finally:
        f.close()

def write_split_files(sql_path, out_dir):
    ensure_dir(out_dir)
    text = read_file(sql_path)
    # best-effort decode; keep bytes if decode fails
    try:
        text_u = text.decode("utf-8")
    except:
        try:
            text_u = text.decode("latin-1")
        except:
            text_u = text  # bytes

    blocks = split_blocks_by_markers(text_u)
    mapping = {}  # key -> (filepath, user)

    for block in blocks:
        extracted = extract_object(block)
        if not extracted:
            continue

        key, user = extracted
        obj_type, obj_name = key

        fname = "%s_%s.sql" % (safe_filename(obj_type.replace(" ", "_")), safe_filename(obj_name))
        fpath = os.path.join(out_dir, fname)

        # avoid overwriting duplicates
        if os.path.exists(fpath):
            i = 2
            while True:
                fname2 = "%s_%s__%d.sql" % (safe_filename(obj_type.replace(" ", "_")), safe_filename(obj_name), i)
                fpath2 = os.path.join(out_dir, fname2)
                if not os.path.exists(fpath2):
                    fpath = fpath2
                    break
                i += 1

        write_file(fpath, block.strip() + "\n")
        mapping[key] = (fpath, user)

    return mapping

def write_mismatch_file(mismatch_dir, key, prod_file, qat_file):
    """
    Writes one mismatch file per object (only when mismatch).
    File contains both Prod and QAT DDL blocks.
    """
    ensure_dir(mismatch_dir)

    obj_type, obj_name = key
    out_name = "%s_%s_MISMATCH.sql" % (safe_filename(obj_type.replace(" ", "_")), safe_filename(obj_name))
    out_path = os.path.join(mismatch_dir, out_name)

    prod_txt = read_file(prod_file)
    qat_txt  = read_file(qat_file)

    # Try to decode for nicer output; fallback to raw bytes.
    def to_text(b):
        try:
            return b.decode("utf-8")
        except:
            try:
                return b.decode("latin-1")
            except:
                return b

    prod_t = to_text(prod_txt)
    qat_t  = to_text(qat_txt)

    combined = (
        "-- =====================================================\n"
        "-- MISMATCH: %s %s\n"
        "-- PROD FILE: %s\n"
        "-- QAT  FILE: %s\n"
        "-- =====================================================\n\n"
        "-- ===================== PROD ==========================\n"
        "%s\n\n"
        "-- ====================== QAT ==========================\n"
        "%s\n"
    ) % (obj_type, obj_name, prod_file, qat_file, prod_t, qat_t)

    write_file(out_path, combined)

def build_csv(prod_map, qat_map, csv_path, mismatch_dir=None):
    keys = set(prod_map.keys()) | set(qat_map.keys())
    keys = sorted(list(keys), key=lambda k: (k[0], k[1]))  # (obj_type, obj_name)

    # Python 2 csv needs binary mode
    f = open(csv_path, "wb")
    try:
        w = csv.writer(f)
        w.writerow(["Prod Filename", "QAT Filename", "Object Type", "User", "Object Name", "Comparison result"])

        for key in keys:
            p = prod_map.get(key)  # (path, user)
            q = qat_map.get(key)

            prod_yes = "Yes" if p else "No"
            qat_yes  = "Yes" if q else "No"

            # user = Prod user if exists else QAT user
            if p:
                user = p[1]
            elif q:
                user = q[1]
            else:
                user = "UNKNOWN"

            if p and q:
                p_txt = read_file(p[0])
                q_txt = read_file(q[0])

                # decode best-effort for normalization
                try:
                    p_u = p_txt.decode("utf-8")
                except:
                    try:
                        p_u = p_txt.decode("latin-1")
                    except:
                        p_u = p_txt
                try:
                    q_u = q_txt.decode("utf-8")
                except:
                    try:
                        q_u = q_txt.decode("latin-1")
                    except:
                        q_u = q_txt

                same = (normalize_sql(p_u) == normalize_sql(q_u))
                cmp_res = "Match" if same else "Mismatch"

                if (not same) and mismatch_dir:
                    write_mismatch_file(mismatch_dir, key, p[0], q[0])

            elif p and (not q):
                cmp_res = "missing in QAT"
            else:
                cmp_res = "missing in PROD"

            w.writerow([prod_yes, qat_yes, key[0], user, key[1], cmp_res])
    finally:
        f.close()

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--prod", required=True, help="Prod export SQL file")
    ap.add_argument("--qat", required=True, help="QAT export SQL file")
    ap.add_argument("--out", default="objects.csv", help="Output CSV file")
    ap.add_argument("--prod-dir", default="Prod", help="Output dir for Prod split files")
    ap.add_argument("--qat-dir", default="QAT", help="Output dir for QAT split files")
    ap.add_argument("--mismatch-dir", default="Diff", help="Folder for mismatch files (only created if mismatches exist)")
    args = ap.parse_args()

    prod_map = write_split_files(args.prod, args.prod_dir)
    qat_map  = write_split_files(args.qat,  args.qat_dir)

    # Only create mismatch dir when we actually write mismatches.
    # We'll pass it in; ensure_dir is called only if mismatch happens.
    mismatch_dir = args.mismatch_dir if args.mismatch_dir else None

    build_csv(prod_map, qat_map, args.out, mismatch_dir=mismatch_dir)

    print "Done."
    print "Prod folder:", os.path.abspath(args.prod_dir)
    print "QAT  folder:", os.path.abspath(args.qat_dir)
    print "CSV:", os.path.abspath(args.out)
    if mismatch_dir:
        print "Mismatch folder (only if mismatches exist):", os.path.abspath(mismatch_dir)

if __name__ == "__main__":
    main()
