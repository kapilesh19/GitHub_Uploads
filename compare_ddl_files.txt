#!/usr/bin/env python3
import argparse
import csv
import re
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional, Tuple

# -----------------------------
# Marker lines
# -----------------------------
START_RE = re.compile(r'^\s*"\s*~~##~~\s*$', re.IGNORECASE) #This is the start of each DDL in the export
END_RE   = re.compile(r'^\s*~~##~~\s*"\s*$', re.IGNORECASE) #This is the end of each DDL in the export

# -----------------------------
# Object header parsing
# -----------------------------
OBJ_HEADER_RE = re.compile(
    r"""^\s*CREATE\s+(?:OR\s+REPLACE\s+)?(?P<otype>PACKAGE\s+BODY|TYPE\s+BODY|PACKAGE|TYPE|VIEW|TABLE|INDEX|SEQUENCE|PROCEDURE|FUNCTION|TRIGGER)\s+
        (?P<oname>(?:"[^"]+"|\w[\w$#]*)(?:\s*\.\s*(?:"[^"]+"|\w[\w$#]*))?)
    """,
    re.IGNORECASE | re.VERBOSE | re.MULTILINE,
)

COMMENT_LINE_RE  = re.compile(r"^\s*--.*?$", re.MULTILINE)
COMMENT_BLOCK_RE = re.compile(r"/\*.*?\*/", re.DOTALL)

# Stripping oracle related environment variables to make comparison more in terms of code

STRIP_PATTERNS = [
    re.compile(r"\bTABLESPACE\b\s+\w[\w$#]*", re.IGNORECASE),
    re.compile(r"\bSTORAGE\b\s*\(.*?\)", re.IGNORECASE | re.DOTALL),
    re.compile(r"\bPCTFREE\b\s+\d+", re.IGNORECASE),
    re.compile(r"\bPCTUSED\b\s+\d+", re.IGNORECASE),
    re.compile(r"\bINITRANS\b\s+\d+", re.IGNORECASE),
    re.compile(r"\bMAXTRANS\b\s+\d+", re.IGNORECASE),
    re.compile(r"\bNOLOGGING\b|\bLOGGING\b", re.IGNORECASE),
    re.compile(r"\bNOPARALLEL\b|\bPARALLEL\b(?:\s+\d+)?", re.IGNORECASE),
    re.compile(r"\bNOCOMPRESS\b|\bCOMPRESS\b(?:\s+\w+)?", re.IGNORECASE),
    re.compile(r"\bSEGMENT\s+CREATION\b\s+(?:IMMEDIATE|DEFERRED)", re.IGNORECASE),
    re.compile(r"\bCACHE\b\s+\d+|\bNOCACHE\b", re.IGNORECASE),
]

def unquote_ident(s: str) -> str:
    s = s.strip()
    return s[1:-1] if s.startswith('"') and s.endswith('"') else s

def split_schema_object(qualified: str) -> Tuple[Optional[str], str]:
    q = qualified.strip()
    if "." in q:
        left, right = [p.strip() for p in q.split(".", 1)]
        return unquote_ident(left), unquote_ident(right)
    return None, unquote_ident(q)

def safe_filename(s: str) -> str:
    return re.sub(r"[^A-Za-z0-9_\-$#]+", "_", s.strip())

def normalize_sql(text: str, strip_physical: bool = True) -> str:
    t = COMMENT_BLOCK_RE.sub("", text)
    t = COMMENT_LINE_RE.sub("", t)
    t = t.replace("\r\n", "\n").replace("\r", "\n")
    if strip_physical:
        for pat in STRIP_PATTERNS:
            t = pat.sub("", t)
    t = re.sub(r"\s+", " ", t).strip()
    return t.upper()

def split_blocks_by_markers(sql_text: str) -> List[str]:
    lines = sql_text.replace("\r\n", "\n").replace("\r", "\n").split("\n")
    blocks: List[str] = []
    buf: List[str] = []
    inside = False

    for line in lines:
        if not inside and START_RE.match(line):
            inside = True
            buf = []
            continue
        if inside and END_RE.match(line):
            block = "\n".join(buf).strip()
            if block:
                blocks.append(block)
            inside = False
            buf = []
            continue
        if inside:
            buf.append(line)

    if inside:
        block = "\n".join(buf).strip()
        if block:
            blocks.append(block)

    return blocks
@dataclass(frozen=True)
class ObjKey:
    obj_type: str
    obj_name: str

def extract_object(block: str) -> Optional[Tuple[ObjKey, str]]:
    m = OBJ_HEADER_RE.search(block)
    if not m:
        return None
    obj_type = re.sub(r"\s+", " ", m.group("otype").strip()).upper()
    qualified = m.group("oname").strip()
    schema, name = split_schema_object(qualified)
    user = schema if schema else "UNKNOWN"
    return ObjKey(obj_type=obj_type, obj_name=name), user

def write_split_files(sql_path: Path, out_dir: Path) -> Dict[ObjKey, Tuple[Path, str]]:
    text = sql_path.read_text(encoding="utf-8", errors="ignore")
    blocks = split_blocks_by_markers(text)
    out_dir.mkdir(parents=True, exist_ok=True)

    mapping: Dict[ObjKey, Tuple[Path, str]] = {}

    for block in blocks:
        extracted = extract_object(block)
        if not extracted:
            continue
        key, user = extracted

        fname = f"{safe_filename(key.obj_type.replace(' ', '_'))}_{safe_filename(key.obj_name)}.sql"
        fpath = out_dir / fname

        if fpath.exists():
            i = 2
            while True:
                fpath2 = out_dir / f"{safe_filename(key.obj_type.replace(' ', '_'))}_{safe_filename(key.obj_name)}__{i}.sql"
                if not fpath2.exists():
                    fpath = fpath2
                    break
                i += 1

        fpath.write_text(block.strip() + "\n", encoding="utf-8", errors="ignore")
        mapping[key] = (fpath, user)

    return mapping

def write_mismatch_file(mismatch_dir: Path, key: ObjKey, prod_file: Path, qat_file: Path) -> None:
   
    #Writes one mismatch file per object
    
    mismatch_dir.mkdir(parents=True, exist_ok=True)

    out_name = f"{safe_filename(key.obj_type.replace(' ', '_'))}_{safe_filename(key.obj_name)}_MISMATCH.sql"
    out_path = mismatch_dir / out_name

    prod_txt = prod_file.read_text(encoding="utf-8", errors="ignore")
    qat_txt  = qat_file.read_text(encoding="utf-8", errors="ignore")

    combined = (
        f"-- =====================================================\n"
        f"-- MISMATCH: {key.obj_type} {key.obj_name}\n"
        f"-- PROD FILE: {prod_file}\n"
        f"-- QAT  FILE: {qat_file}\n"
        f"-- =====================================================\n\n"
        f"-- ===================== PROD ==========================\n"
        f"{prod_txt}\n\n"
        f"-- ====================== QAT ==========================\n"
        f"{qat_txt}\n"
    )

    out_path.write_text(combined, encoding="utf-8", errors="ignore")

def build_csv(prod_map: Dict[ObjKey, Tuple[Path, str]],
              qat_map: Dict[ObjKey, Tuple[Path, str]],
              csv_path: Path,
              mismatch_dir: Optional[Path] = None) -> None:

    all_keys = sorted(set(prod_map) | set(qat_map), key=lambda k: (k.obj_type, k.obj_name))

    with csv_path.open("w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(["Prod Filename", "QAT Filename", "Object Type", "User", "Object Name", "Comparison result"])

        for key in all_keys:
            p = prod_map.get(key)  # (path, user)
            q = qat_map.get(key)

            prod_yes = "Yes" if p else "No"
            qat_yes  = "Yes" if q else "No"

            # User column = Prod user if exists else QAT user
            user = (p[1] if p else q[1]) if (p or q) else "UNKNOWN"

            if p and q:
                p_txt = p[0].read_text(encoding="utf-8", errors="ignore")
                q_txt = q[0].read_text(encoding="utf-8", errors="ignore")
                same = normalize_sql(p_txt) == normalize_sql(q_txt)
                cmp = "Match" if same else "Mismatch"

                # only create mismatch file when different AND mismatch_dir provided
                if (not same) and mismatch_dir is not None:
                    write_mismatch_file(mismatch_dir, key, p[0], q[0])

            elif p and not q:
                cmp = "missing in QAT"
            else:
                cmp = "missing in PROD"

            w.writerow([prod_yes, qat_yes, key.obj_type, user, key.obj_name, cmp])

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--prod", required=True)
    ap.add_argument("--qat", required=True)
    ap.add_argument("--out", default="objects.csv")
    ap.add_argument("--prod-dir", default="Prod")
    ap.add_argument("--qat-dir", default="QAT")
    ap.add_argument("--mismatch-dir", default="Diff", help="Folder to write mismatch files (one per object). Created only when mismatches exist.")

    args = ap.parse_args()

    prod_map = write_split_files(Path(args.prod), Path(args.prod_dir))
    qat_map  = write_split_files(Path(args.qat),  Path(args.qat_dir))

    mismatch_dir = Path(args.mismatch_dir) if args.mismatch_dir else None

    build_csv(prod_map, qat_map, Path(args.out), mismatch_dir=mismatch_dir)

    print("Done.")
    print(f"Prod folder: {Path(args.prod_dir).resolve()}")
    print(f"QAT  folder: {Path(args.qat_dir).resolve()}")
    print(f"CSV: {Path(args.out).resolve()}")
    if mismatch_dir:
        print(f"Mismatch files folder: {mismatch_dir.resolve()} (only mismatches are written)")

if __name__ == "__main__":
    main()
